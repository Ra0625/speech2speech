{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work with non-parallel voice conversion. This is more challenging but more valuable given that it s difficult to collect parallel training data of different speakers. \n",
    "\n",
    "There are two ways in which non-parallel data is handeled:\n",
    "1. Convert it into parallel data and then learn mappining functions like \n",
    "    1.1 generatinf parallel data through test-to-speech synthesis \n",
    "    1.2 frame selection\n",
    "    1.3 iterative combination of a nearest neighbor search step and a conversion stem alignment \n",
    "    1.4 CycleGAN based Voice conversion \n",
    "2. Factorize linguistic and speaker related representations carried by acoustic features \n",
    "    At conversion stage the linguistic content of the source speaker is tranformed to that of targer speaker \n",
    "    \n",
    "\n",
    "References https://arxiv.org/pdf/1906.10508.pdf\n",
    "\n",
    "\n",
    "We work with raw speech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:32:35.055234Z",
     "start_time": "2020-01-22T18:32:34.069236Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4cc6282b290e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mipd\u001b[0m  \u001b[0;31m# To play sound in the notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import pandas as pd       \n",
    "import os \n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "from six.moves import xrange\n",
    "\n",
    "import umap\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T20:46:23.928833Z",
     "start_time": "2020-01-22T20:46:23.920992Z"
    }
   },
   "outputs": [],
   "source": [
    "# PATHS \n",
    "\n",
    "raw_data = '/home/ubuntu/voice_conversion/data/raw/VCTK-Corpus'\n",
    "interim_data = os.path.join('/home/ubuntu/voice_conversion/data', 'interim')\n",
    "spectogram_array_path =  os.path.join(interim_data, 'spectogram_array')\n",
    "spectrogram_path =os.path.join(interim_data, 'spectogram') \n",
    "audio_path = '/home/ubuntu/voice_conversion/data/raw/VCTK-Corpus/wav48/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MKDIR\n",
    "os.mkdir('/home/ubuntu/voice_conversion/data/interim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(spectogram_array_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:32:38.829920Z",
     "start_time": "2020-01-22T18:32:38.704012Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPYING  NOTE  README  speaker-info.txt  txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T19:43:04.042115Z",
     "start_time": "2020-01-22T19:43:04.004671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'AGE', 'GENDER', 'ACCENTS', 'REGION'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>AGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>108.000000</td>\n",
       "      <td>108.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>286.120370</td>\n",
       "      <td>22.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>39.629763</td>\n",
       "      <td>3.013986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>225.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>253.750000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>281.500000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>313.250000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>376.000000</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID         AGE\n",
       "count  108.000000  108.000000\n",
       "mean   286.120370   22.666667\n",
       "std     39.629763    3.013986\n",
       "min    225.000000   18.000000\n",
       "25%    253.750000   21.000000\n",
       "50%    281.500000   22.000000\n",
       "75%    313.250000   23.000000\n",
       "max    376.000000   38.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(raw_data,\"speaker-info.txt\"),delimiter= '\\s+', index_col=False)\n",
    "print(df.columns)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T19:51:57.380204Z",
     "start_time": "2020-01-22T19:51:57.372674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>AGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>ACCENTS</th>\n",
       "      <th>REGION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Southern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>226</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>English</td>\n",
       "      <td>Surrey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>227</td>\n",
       "      <td>38</td>\n",
       "      <td>M</td>\n",
       "      <td>English</td>\n",
       "      <td>Cumbria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>228</td>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Southern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>229</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Southern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>230</td>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Stockton-on-tees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>231</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Southern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>232</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>English</td>\n",
       "      <td>Southern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>233</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Staffordshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>234</td>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>Scottish</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  AGE GENDER   ACCENTS            REGION\n",
       "0  225   23      F   English          Southern\n",
       "1  226   22      M   English            Surrey\n",
       "2  227   38      M   English           Cumbria\n",
       "3  228   22      F   English          Southern\n",
       "4  229   23      F   English          Southern\n",
       "5  230   22      F   English  Stockton-on-tees\n",
       "6  231   23      F   English          Southern\n",
       "7  232   23      M   English          Southern\n",
       "8  233   23      F   English     Staffordshire\n",
       "9  234   22      F  Scottish              West"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T19:44:09.315972Z",
     "start_time": "2020-01-22T19:44:09.146714Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2400c58c88>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['ACCENTS'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data contains a very low number of Indian accent English speakers, which in reality is different. \n",
    "Source: https://en.wikipedia.org/wiki/List_of_countries_by_English-speaking_population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T18:25:55.702736Z",
     "start_time": "2020-01-21T18:25:55.700406Z"
    }
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T20:51:35.439578Z",
     "start_time": "2020-01-22T20:51:35.416697Z"
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import argparse\n",
    "import pyworld\n",
    "from multiprocessing import cpu_count\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "from os.path import join, basename\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T20:40:27.573680Z",
     "start_time": "2020-01-22T20:40:27.553041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{225: 'English',\n",
       " 226: 'English',\n",
       " 227: 'English',\n",
       " 228: 'English',\n",
       " 229: 'English',\n",
       " 230: 'English',\n",
       " 231: 'English',\n",
       " 232: 'English',\n",
       " 233: 'English',\n",
       " 234: 'Scottish',\n",
       " 236: 'English',\n",
       " 237: 'Scottish',\n",
       " 238: 'NorthernIrish',\n",
       " 239: 'English',\n",
       " 240: 'English',\n",
       " 241: 'Scottish',\n",
       " 243: 'English',\n",
       " 244: 'English',\n",
       " 245: 'Irish',\n",
       " 246: 'Scottish',\n",
       " 247: 'Scottish',\n",
       " 248: 'Indian',\n",
       " 249: 'Scottish',\n",
       " 250: 'English',\n",
       " 251: 'Indian',\n",
       " 252: 'Scottish',\n",
       " 253: 'Welsh',\n",
       " 254: 'English',\n",
       " 255: 'Scottish',\n",
       " 256: 'English',\n",
       " 257: 'English',\n",
       " 258: 'English',\n",
       " 259: 'English',\n",
       " 260: 'Scottish',\n",
       " 261: 'NorthernIrish',\n",
       " 262: 'Scottish',\n",
       " 263: 'Scottish',\n",
       " 264: 'Scottish',\n",
       " 265: 'Scottish',\n",
       " 266: 'Irish',\n",
       " 267: 'English',\n",
       " 268: 'English',\n",
       " 269: 'English',\n",
       " 270: 'English',\n",
       " 271: 'Scottish',\n",
       " 272: 'Scottish',\n",
       " 273: 'English',\n",
       " 274: 'English',\n",
       " 275: 'Scottish',\n",
       " 276: 'English',\n",
       " 277: 'English',\n",
       " 278: 'English',\n",
       " 279: 'English',\n",
       " 281: 'Scottish',\n",
       " 282: 'English',\n",
       " 283: 'Irish',\n",
       " 284: 'Scottish',\n",
       " 285: 'Scottish',\n",
       " 286: 'English',\n",
       " 287: 'English',\n",
       " 288: 'Irish',\n",
       " 292: 'NorthernIrish',\n",
       " 293: 'NorthernIrish',\n",
       " 294: 'American',\n",
       " 295: 'Irish',\n",
       " 297: 'American',\n",
       " 298: 'Irish',\n",
       " 299: 'American',\n",
       " 300: 'American',\n",
       " 301: 'American',\n",
       " 302: 'Canadian',\n",
       " 303: 'Canadian',\n",
       " 304: 'NorthernIrish',\n",
       " 305: 'American',\n",
       " 306: 'American',\n",
       " 307: 'Canadian',\n",
       " 308: 'American',\n",
       " 310: 'American',\n",
       " 311: 'American',\n",
       " 312: 'Canadian',\n",
       " 313: 'Irish',\n",
       " 314: 'SouthAfrican',\n",
       " 315: 'American',\n",
       " 316: 'Canadian',\n",
       " 317: 'Canadian',\n",
       " 318: 'American',\n",
       " 323: 'SouthAfrican',\n",
       " 326: 'Australian',\n",
       " 329: 'American',\n",
       " 330: 'American',\n",
       " 333: 'American',\n",
       " 334: 'American',\n",
       " 335: 'NewZealand',\n",
       " 336: 'SouthAfrican',\n",
       " 339: 'American',\n",
       " 340: 'Irish',\n",
       " 341: 'American',\n",
       " 343: 'Canadian',\n",
       " 345: 'American',\n",
       " 347: 'SouthAfrican',\n",
       " 351: 'NorthernIrish',\n",
       " 360: 'American',\n",
       " 361: 'American',\n",
       " 362: 'American',\n",
       " 363: 'Canadian',\n",
       " 364: 'Irish',\n",
       " 374: 'Australian',\n",
       " 376: 'Indian'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below is the accent information \n",
    "spk2acc = pd.Series(df.ACCENTS.values,index=df.ID).to_dict()\n",
    "spk2acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = 256   # Since we slice 256 frames from each utterance when training.\n",
    "# Build a dict useful when we want to get one-hot representation of speakers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T22:19:42.855094Z",
     "start_time": "2020-01-22T22:19:42.852376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bithika/reSpeecher_voice_conversion/data/raw/VCTK-Corpus'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T23:14:49.102015Z",
     "start_time": "2020-01-22T23:14:48.657638Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'librosa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4bcc62a8a56d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wav'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'librosa' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "files = librosa.util.find_files(raw_data, ext=['wav']) \n",
    "files = np.asarray(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T23:14:01.170283Z",
     "start_time": "2020-01-22T23:14:01.166217Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_spectrogram_array():\n",
    "    #print(f'Processing fold {fold}')\n",
    "    #os.mkdir(os.path.join(spectrogram_path,fold))\n",
    "    for audio_file in list(files):\n",
    "        samples, sample_rate = librosa.load(audio_file)\n",
    "        audio_name = audio_file.split('raw')[1].split('/')[-1].split('.')[0]\n",
    "        filename  = spectrogram_path + audio_name + '.png'\n",
    "        X = librosa.stft(samples)\n",
    "        Xdb = librosa.amplitude_to_db(abs(X))\n",
    "        np.save(filename, Xbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T23:25:36.936651Z",
     "start_time": "2020-01-22T23:25:36.916750Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_fold_spectrograms():\n",
    "    spectrogram_path ='/home/bithika/reSpeecher_voice_conversion/data/interim/spectrogram/' \n",
    "    audio_path = '/home/bithika/reSpeecher_voice_conversion/data/raw/VCTK-Corpus/wav48/'\n",
    "    #print(f'Processing fold {fold}')\n",
    "    #os.mkdir(os.path.join(spectrogram_path,fold))\n",
    "    for audio_file in list(files):\n",
    "        samples, sample_rate = librosa.load(audio_file)\n",
    "        fig = plt.figure(figsize=[0.72,0.72])\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        audio_name = audio_file.split('raw')[1].split('/')[-1].split('.')[0]\n",
    "        filename  = spectrogram_path + audio_name + '.png'\n",
    "        S = librosa.feature.melspectrogram(y=samples, sr=sample_rate)\n",
    "        librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n",
    "        plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n",
    "        plt.close('all')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T23:40:57.057952Z",
     "start_time": "2020-01-22T23:25:38.905706Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-c36e547813c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_fold_spectrograms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-205-681e6e04b5cf>\u001b[0m in \u001b[0;36mcreate_fold_spectrograms\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#os.mkdir(os.path.join(spectrogram_path,fold))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0maudio_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.72\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.72\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mgcd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_sr\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mgcd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresampy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/resampy/core.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(x, sr_orig, sr_new, axis, filter, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mx_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0my_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mresample_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp_win\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_fold_spectrograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(spk, origin_wavpath, target_wavpath):\n",
    "    wavfiles = [i for i in os.listdir(join(origin_wavpath, spk)) if i.endswith(\".wav\")]\n",
    "    for wav in wavfiles:\n",
    "        folder_to = join(target_wavpath, spk)\n",
    "        os.makedirs(folder_to, exist_ok=True)\n",
    "        wav_to = join(folder_to, wav)\n",
    "        wav_from = join(origin_wavpath, spk, wav)\n",
    "        subprocess.call(['sox', wav_from, \"-r\", \"16000\", wav_to])\n",
    "    return 0\n",
    "\n",
    "def resample_to_16k(origin_wavpath, target_wavpath, num_workers=1):\n",
    "    os.makedirs(target_wavpath, exist_ok=True)\n",
    "    spk_folders = os.listdir(origin_wavpath)\n",
    "    print(f\"> Using {num_workers} workers!\")\n",
    "    executor = ProcessPoolExecutor(max_workers=num_workers)\n",
    "    futures = []\n",
    "    for spk in spk_folders:\n",
    "        futures.append(executor.submit(partial(resample, spk, origin_wavpath, target_wavpath)))\n",
    "    result_list = [future.result() for future in tqdm(futures)]\n",
    "    print(result_list)\n",
    "\n",
    "def split_data(paths):\n",
    "    indices = np.arange(len(paths))\n",
    "    test_size = 0.1\n",
    "    train_indices, test_indices = train_test_split(indices, test_size=test_size, random_state=1234)\n",
    "    train_paths = list(np.array(paths)[train_indices])\n",
    "    test_paths = list(np.array(paths)[test_indices])\n",
    "    return train_paths, test_paths\n",
    "\n",
    "def get_spk_world_feats(spk_fold_path, mc_dir_train, mc_dir_test, sample_rate=16000):\n",
    "    paths = glob.glob(join(spk_fold_path, '*.wav'))\n",
    "    spk_name = basename(spk_fold_path)\n",
    "    train_paths, test_paths = split_data(paths)\n",
    "    f0s = []\n",
    "    coded_sps = []\n",
    "    for wav_file in train_paths:\n",
    "        f0, _, _, _, coded_sp = world_encode_wav(wav_file, fs=sample_rate)\n",
    "        f0s.append(f0)\n",
    "        coded_sps.append(coded_sp)\n",
    "    log_f0s_mean, log_f0s_std = logf0_statistics(f0s)\n",
    "    coded_sps_mean, coded_sps_std = coded_sp_statistics(coded_sps)\n",
    "    np.savez(join(mc_dir_train, spk_name+'_stats.npz'), \n",
    "            log_f0s_mean=log_f0s_mean,\n",
    "            log_f0s_std=log_f0s_std,\n",
    "            coded_sps_mean=coded_sps_mean,\n",
    "            coded_sps_std=coded_sps_std)\n",
    "    \n",
    "    for wav_file in tqdm(train_paths):\n",
    "        wav_nam = basename(wav_file)\n",
    "        f0, timeaxis, sp, ap, coded_sp = world_encode_wav(wav_file, fs=sample_rate)\n",
    "        normed_coded_sp = normalize_coded_sp(coded_sp, coded_sps_mean, coded_sps_std)\n",
    "        np.save(join(mc_dir_train, wav_nam.replace('.wav', '.npy')), normed_coded_sp, allow_pickle=False)\n",
    "    \n",
    "    for wav_file in tqdm(test_paths):\n",
    "        wav_nam = basename(wav_file)\n",
    "        f0, timeaxis, sp, ap, coded_sp = world_encode_wav(wav_file, fs=sample_rate)\n",
    "        normed_coded_sp = normalize_coded_sp(coded_sp, coded_sps_mean, coded_sps_std)\n",
    "        np.save(join(mc_dir_test, wav_nam.replace('.wav', '.npy')), normed_coded_sp, allow_pickle=False)\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "    sample_rate_default = 16000\n",
    "    origin_wavpath_default = \"./data/VCTK-Corpus/wav48\"\n",
    "    target_wavpath_default = \"./data/VCTK-Corpus/wav16\"\n",
    "    mc_dir_train_default = './data/mc/train'\n",
    "    mc_dir_test_default = './data/mc/test'\n",
    "\n",
    "    parser.add_argument(\"--sample_rate\", type = int, default = 16000, help = \"Sample rate.\")\n",
    "    parser.add_argument(\"--origin_wavpath\", type = str, default = origin_wavpath_default, help = \"The original wav path to resample.\")\n",
    "    parser.add_argument(\"--target_wavpath\", type = str, default = target_wavpath_default, help = \"The original wav path to resample.\")\n",
    "    parser.add_argument(\"--mc_dir_train\", type = str, default = mc_dir_train_default, help = \"The directory to store the training features.\")\n",
    "    parser.add_argument(\"--mc_dir_test\", type = str, default = mc_dir_test_default, help = \"The directory to store the testing features.\")\n",
    "    parser.add_argument(\"--num_workers\", type = int, default = None, help = \"The number of cpus to use.\")\n",
    "\n",
    "    argv = parser.parse_args()\n",
    "\n",
    "    sample_rate = argv.sample_rate\n",
    "    origin_wavpath = argv.origin_wavpath\n",
    "    target_wavpath = argv.target_wavpath\n",
    "    mc_dir_train = argv.mc_dir_train\n",
    "    mc_dir_test = argv.mc_dir_test\n",
    "    num_workers = argv.num_workers if argv.num_workers is not None else cpu_count()\n",
    "\n",
    "    # The original wav in VCTK is 48K, first we want to resample to 16K\n",
    "    resample_to_16k(origin_wavpath, target_wavpath, num_workers=num_workers)\n",
    "\n",
    "    # WE only use 10 speakers listed below for this experiment.\n",
    "    speaker_used = ['262', '272', '229', '232', '292', '293', '360', '361', '248', '251']\n",
    "    speaker_used = ['p'+i for i in speaker_used]\n",
    "\n",
    "    ## Next we are to extract the acoustic features (MCEPs, lf0) and compute the corresponding stats (means, stds). \n",
    "    # Make dirs to contain the MCEPs\n",
    "    os.makedirs(mc_dir_train, exist_ok=True)\n",
    "    os.makedirs(mc_dir_test, exist_ok=True)\n",
    "\n",
    "    num_workers = len(speaker_used) #cpu_count()\n",
    "    print(\"number of workers: \", num_workers)\n",
    "    executor = ProcessPoolExecutor(max_workers=num_workers)\n",
    "\n",
    "    work_dir = target_wavpath\n",
    "    # spk_folders = os.listdir(work_dir)\n",
    "    # print(\"processing {} speaker folders\".format(len(spk_folders)))\n",
    "    # print(spk_folders)\n",
    "\n",
    "    futures = []\n",
    "    for spk in speaker_used:\n",
    "        spk_path = os.path.join(work_dir, spk)\n",
    "        futures.append(executor.submit(partial(get_spk_world_feats, spk_path, mc_dir_train, mc_dir_test, sample_rate)))\n",
    "    result_list = [future.result() for future in tqdm(futures)]\n",
    "    print(result_list)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language is inherently discrete, similarly speech is typically representated as a sequence of symbols. thus it makes sense to focus on Learning representation with discrete representations. \n",
    "\n",
    "VQVAE\n",
    "pros: \n",
    "1. does not suffer from large variance \n",
    "2. avoids [posterior collapse](https://datascience.stackexchange.com/questions/48962/what-is-posterior-collapse-phenomenon)\n",
    "\n",
    "VQ-VAE uses latent space in an effective manner: it can model important features which span many dimensions in data space (like phonemes in speech) as opposed to focusing or spending capacity on noise and imperceptible details which are oftern local\n",
    "\n",
    "Lastly, once a good discrete latent structure of modality is discovered by VQ VAE we train a powerful prior over theses discrete random variables, which can yield interesting samples and useful applications \n",
    "\n",
    "When trained on speech one can discover latent structure of language without any supervision or prior knowledge about phonemes or words.  Furthermore we can equip decoder with the speaker identity which could allow for speaker conversion i.e transferrrinf voice from one speaker to another without changing contents. \n",
    "\n",
    "\n",
    "Main contribution of VQ VAE \n",
    "- when paired with a powerful proir, samples are coherent and hight quality speech generation\n",
    "- their is some evidence of learning language through raw speech without any supervision and show applications of unsupervised speaker conversion\n",
    "\n",
    "For speech one actually extracts 1D latent features \n",
    "\n",
    "\n",
    "\n",
    "https://blog.usejournal.com/understanding-vector-quantized-variational-autoencoders-vq-vae-323d710a888a\n",
    "\n",
    "Ref [ELBO](https://medium.com/@hfdtsinghua/derivation-of-elbo-in-vae-25ad7991fdf7)\n",
    "\n",
    "\n",
    "![Model](VQ_VAE.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T23:43:56.983393Z",
     "start_time": "2020-01-21T23:43:56.935490Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio.datasets.utils import download_url, extract_archive, walk_files\n",
    "\n",
    "URL = \"http://homepages.inf.ed.ac.uk/jyamagis/release/VCTK-Corpus.tar.gz\"\n",
    "FOLDER_IN_ARCHIVE = \"VCTK-Corpus\"\n",
    "\n",
    "\n",
    "def load_vctk_item(\n",
    "    fileid, path, ext_audio, ext_txt, folder_audio, folder_txt, downsample=False\n",
    "):\n",
    "    speaker_id, utterance_id = fileid.split(\"_\")\n",
    "\n",
    "    # Read text\n",
    "    file_txt = os.path.join(path, folder_txt, speaker_id, fileid + ext_txt)\n",
    "    with open(file_txt) as file_text:\n",
    "        utterance = file_text.readlines()[0]\n",
    "\n",
    "    # Read wav\n",
    "    file_audio = os.path.join(path, folder_audio, speaker_id, fileid + ext_audio)\n",
    "    waveform, sample_rate = torchaudio.load(file_audio)\n",
    "    if downsample:\n",
    "        # TODO Remove this parameter after deprecation\n",
    "        F = torchaudio.functional\n",
    "        T = torchaudio.transforms\n",
    "        # rate\n",
    "        sample = T.Resample(sample_rate, 16000, resampling_method='sinc_interpolation')\n",
    "        waveform = sample(waveform)\n",
    "        # dither\n",
    "        waveform = F.dither(waveform, noise_shaping=True)\n",
    "\n",
    "    return waveform, sample_rate, utterance, speaker_id, utterance_id\n",
    "\n",
    "\n",
    "class VCTK(Dataset):\n",
    "    \"\"\"\n",
    "    Create a Dataset for VCTK. Each item is a tuple of the form:\n",
    "    (waveform, sample_rate, utterance, speaker_id, utterance_id)\n",
    "    \"\"\"\n",
    "\n",
    "    _folder_txt = \"txt\"\n",
    "    _folder_audio = \"wav48\"\n",
    "    _ext_txt = \".txt\"\n",
    "    _ext_audio = \".wav\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        url=URL,\n",
    "        folder_in_archive=FOLDER_IN_ARCHIVE,\n",
    "        download=False,\n",
    "        downsample=False,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "    ):\n",
    "\n",
    "        if downsample:\n",
    "            warnings.warn(\n",
    "                \"In the next version, transforms will not be part of the dataset. \"\n",
    "                \"Please use `downsample=False` to enable this behavior now, \",\n",
    "                \"and suppress this warning.\",\n",
    "                DeprecationWarning,\n",
    "            )\n",
    "\n",
    "        if transform is not None or target_transform is not None:\n",
    "            warnings.warn(\n",
    "                \"In the next version, transforms will not be part of the dataset. \"\n",
    "                \"Please remove the option `transform=True` and \"\n",
    "                \"`target_transform=True` to suppress this warning.\",\n",
    "                DeprecationWarning,\n",
    "            )\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        archive = os.path.basename(url)\n",
    "        archive = os.path.join(root, archive)\n",
    "        self._path = os.path.join(root, folder_in_archive)\n",
    "\n",
    "        if download:\n",
    "            if not os.path.isdir(self._path):\n",
    "                if not os.path.isfile(archive):\n",
    "                    download_url(url, root)\n",
    "                extract_archive(archive)\n",
    "\n",
    "        if not os.path.isdir(self._path):\n",
    "            raise RuntimeError(\n",
    "                \"Dataset not found. Please use `download=True` to download it.\"\n",
    "            )\n",
    "\n",
    "        walker = walk_files(\n",
    "            self._path, suffix=self._ext_audio, prefix=False, remove_suffix=True\n",
    "        )\n",
    "        self._walker = list(walker)\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        fileid = self._walker[n]\n",
    "        item = load_vctk_item(\n",
    "            fileid,\n",
    "            self._path,\n",
    "            self._ext_audio,\n",
    "            self._ext_txt,\n",
    "            self._folder_audio,\n",
    "            self._folder_txt,\n",
    "        )\n",
    "\n",
    "        # TODO Upon deprecation, uncomment line below and remove following code\n",
    "        # return item\n",
    "\n",
    "        waveform, sample_rate, utterance, speaker_id, utterance_id = item\n",
    "        if self.transform is not None:\n",
    "            waveform = self.transform(waveform)\n",
    "        if self.target_transform is not None:\n",
    "            utterance = self.target_transform(utterance)\n",
    "        return waveform, sample_rate, utterance, speaker_id, utterance_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._walker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
