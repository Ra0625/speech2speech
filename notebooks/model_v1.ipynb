{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd       \n",
    "import os \n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "from six.moves import xrange\n",
    "\n",
    "import umap\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchsummary import summary\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T20:46:23.928833Z",
     "start_time": "2020-01-22T20:46:23.920992Z"
    }
   },
   "outputs": [],
   "source": [
    "# PATHS \n",
    "\n",
    "raw_data = '/home/ubuntu/voice_conversion/data/raw/VCTK-Corpus'\n",
    "interim_data = os.path.join('/home/ubuntu/voice_conversion/data', 'interim')\n",
    "spectogram_array_path =  os.path.join(interim_data, 'spectogram_array')\n",
    "spectrogram_path =os.path.join(interim_data, 'spectogram') \n",
    "audio_path = '/home/ubuntu/voice_conversion/data/raw/VCTK-Corpus/wav48/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/home/ubuntu/voice_conversion/data/interim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ffb4921c88be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# MKDIR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/ubuntu/voice_conversion/data/interim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/home/ubuntu/voice_conversion/data/interim'"
     ]
    }
   ],
   "source": [
    "# MKDIR\n",
    "os.mkdir('/home/ubuntu/voice_conversion/data/interim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(spectogram_array_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "files_np = list(glob.glob(os.path.join(spectogram_array_path,'*.*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "Y_list = []\n",
    "for file in files_np[:100]: \n",
    "    trial_x = np.load(file)\n",
    "    trial_x =torch.tensor(trial_x, device=device).float()\n",
    "    trial_y = file.split('spectogram_array/')[1].split('_')[0][-3:]\n",
    "    trial_y =torch.tensor(int(trial_y), device=device).float()\n",
    "    X_list.append(trial_x)\n",
    "    Y_list.append(trial_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "nX_list = []\n",
    "col_shape = []\n",
    "for file in files_np[:100]: \n",
    "    trial_x = np.load(file)\n",
    "    _, cols = trial_x.shape\n",
    "    nX_list.append(trial_x)\n",
    "    col_shape.append(cols)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(col_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,a in enumerate(nX_list):\n",
    "  rows, cols = a.shape\n",
    "  #print(rows, cols)\n",
    "  if cols != 326:\n",
    "    nX_list[i] = np.hstack([a, np.zeros(( rows, 326 - cols), dtype=a.dtype)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "for file in nX_list: \n",
    "    trial_x =torch.tensor(file, device=device).float()\n",
    "    X_list.append(trial_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = nX_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_Tensor= torch.stack(X_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Tensor= torch.stack(Y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tensordataset = torch.utils.data.TensorDataset(x_Tensor, y_Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(training_tensordataset, batch_size= 10 , shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Quantizer Layer\n",
    "\n",
    "This layer takes a tensor to be quantized. The channel dimension will be used as the space in which to quantize. All other dimensions will be flattened and will be seen as different examples to quantize.\n",
    "\n",
    "The output tensor will have the same shape as the input.\n",
    "\n",
    "As an example for a `BCHW` tensor of shape `[16, 64, 32, 32]`, we will first convert it to an `BHWC` tensor of shape `[16, 32, 32, 64]` and then reshape it into `[16384, 64]` and all `16384` vectors of size `64`  will be quantized independently. In otherwords, the channels are used as the space in which to quantize. All other dimensions will be flattened and be seen as different examples to quantize, `16384` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_training_updates = 1500\n",
    "\n",
    "# Encoder\n",
    "num_hiddens = 768\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "# input_dim: 256\n",
    "\n",
    "# VQ\n",
    "# This value is not that important, usually 64 works.\n",
    "# This will not change the capacity in the information-bottleneck.\n",
    "embedding_dim = 64\n",
    "# The higher this value, the higher the capacity in the information bottleneck.\n",
    "num_embeddings = 29\n",
    "\n",
    "commitment_cost = 0.25\n",
    "\n",
    "decay = -0.99\n",
    "\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, device):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
    "        self._commitment_cost = commitment_cost\n",
    "        self._device = device\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        inputs = inputs.permute(1, 2, 0).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        print('shape of inputs in VectorQuantizer.forward',inputs.shape)\n",
    "        _, time, batch_size = input_shape\n",
    "        \n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        print('shape of flat_input in VectorQuantizer.forward',flat_input.size())\n",
    "        print('device of flat_input',flat_input.device)\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "            \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "        print('shape of quantized in VectorQuantizer.forward',quantized.size())\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        #print(\"with inputs quantized: \"quantized.size())\n",
    "        \n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(2, 0, 1).contiguous(), perplexity, encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also implement a slightly modified version  which will use exponential moving averages to update the embedding vectors instead of an auxillary loss. This has the advantage that the embedding updates are independent of the choice of optimizer for the encoder, decoder and other parts of the architecture. For most experiments the EMA version trains faster than the non-EMA version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizerEMA(nn.Module):\n",
    "    \"\"\"\n",
    "    Inspired from Sonnet implementation of VQ-VAE https://arxiv.org/abs/1711.00937,\n",
    "    in https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py and\n",
    "    pytorch implementation of it from zalandoresearch in https://github.com/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb.\n",
    "    Implements a slightly modified version of the algorithm presented in\n",
    "    'Neural Discrete Representation Learning' by van den Oord et al.\n",
    "    https://arxiv.org/abs/1711.00937\n",
    "    The difference between VectorQuantizerEMA and VectorQuantizer is that\n",
    "    this module uses exponential moving averages to update the embedding vectors\n",
    "    instead of an auxiliary loss. This has the advantage that the embedding\n",
    "    updates are independent of the choice of optimizer (SGD, RMSProp, Adam, K-Fac,\n",
    "    ...) used for the encoder, decoder and other parts of the architecture. For\n",
    "    most experiments the EMA version trains faster than the non-EMA version.\n",
    "    Input any tensor to be quantized. Last dimension will be used as space in\n",
    "    which to quantize. All other dimensions will be flattened and will be seen\n",
    "    as different examples to quantize.\n",
    "    The output tensor will have the same shape as the input.\n",
    "    For example a tensor with shape [16, 32, 32, 64] will be reshaped into\n",
    "    [16384, 64] and all 16384 vectors (each of 64 dimensions)  will be quantized\n",
    "    independently.\n",
    "    Args:\n",
    "        embedding_dim: integer representing the dimensionality of the tensors in the\n",
    "            quantized space. Inputs to the modules must be in this format as well.\n",
    "        num_embeddings: integer, the number of vectors in the quantized space.\n",
    "            commitment_cost: scalar which controls the weighting of the loss terms (see\n",
    "            equation 4 in the paper).\n",
    "        decay: float, decay for the moving averages.\n",
    "        epsilon: small float constant to avoid numerical instability.\n",
    "    \"\"\"\n",
    "        \n",
    "        \n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, device, epsilon=1e-5):\n",
    "           \n",
    "        super(VectorQuantizerEMA, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.normal_()\n",
    "        self._commitment_cost = commitment_cost\n",
    "        \n",
    "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
    "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
    "        self._ema_w.data.normal_()\n",
    "        \n",
    "        self._decay = decay\n",
    "        self._epsilon = epsilon\n",
    "        self._device = device\n",
    "\n",
    "    def forward(self, inputs, compute_distances_if_possible=True, record_codebook_stats=False):\n",
    "        \"\"\"\n",
    "        Connects the module to some inputs.\n",
    "        Args:\n",
    "            inputs: Tensor, final dimension must be equal to embedding_dim. All other\n",
    "                leading dimensions will be flattened and treated as a large batch.\n",
    "        \n",
    "        Returns:\n",
    "            loss: Tensor containing the loss to optimize.\n",
    "            quantize: Tensor containing the quantized version of the input.\n",
    "            perplexity: Tensor containing the perplexity of the encodings.\n",
    "            encodings: Tensor containing the discrete encodings, ie which element\n",
    "                of the quantized space each input element was mapped to.\n",
    "            distances\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        inputs = inputs.permute(1, 2, 0).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "        \"\"\"\n",
    "        encoding_indices: Tensor containing the discrete encoding indices, ie\n",
    "        which element of the quantized space each input element was mapped to.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, dtype=torch.float).to(self._device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Compute distances between encoding vectors\n",
    "        if not self.training and compute_distances_if_possible:\n",
    "            _encoding_distances = [torch.dist(items[0], items[1], 2).to(self._device) for items in combinations(flat_input, r=2)]\n",
    "            encoding_distances = torch.tensor(_encoding_distances).to(self._device).view(batch_size, -1)\n",
    "        else:\n",
    "            encoding_distances = None\n",
    "\n",
    "        # Compute distances between embedding vectors\n",
    "        if not self.training and compute_distances_if_possible:\n",
    "            _embedding_distances = [torch.dist(items[0], items[1], 2).to(self._device) for items in combinations(self._embedding.weight, r=2)]\n",
    "            embedding_distances = torch.tensor(_embedding_distances).to(self._device)\n",
    "        else:\n",
    "            embedding_distances = None\n",
    "\n",
    "        # Sample nearest embedding\n",
    "        if not self.training and compute_distances_if_possible:\n",
    "            _frames_vs_embedding_distances = [torch.dist(items[0], items[1], 2).to(self._device) for items in product(flat_input, self._embedding.weight.detach())]\n",
    "            frames_vs_embedding_distances = torch.tensor(_frames_vs_embedding_distances).to(self._device).view(batch_size, time, -1)\n",
    "        else:\n",
    "            frames_vs_embedding_distances = None\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # Use EMA to update the embedding vectors\n",
    "        if self.training:\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
    "            \n",
    "            # Laplace smoothing of the cluster size\n",
    "            n = torch.sum(self._ema_cluster_size.data)\n",
    "            self._ema_cluster_size = (\n",
    "                (self._ema_cluster_size + self._epsilon)\n",
    "                / (n + self._num_embeddings * self._epsilon) * n)\n",
    "            \n",
    "            dw = torch.matmul(encodings.t(), flat_input)\n",
    "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "            \n",
    "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "        \n",
    "        concatenated_quantized = self._embedding.weight[torch.argmin(distances, dim=1).detach().cpu()] if not self.training or record_codebook_stats else None\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = torch.mean((quantized.detach() - inputs)**2)\n",
    "        loss = self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        # Straight Through Estimator\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        \"\"\"\n",
    "        The perplexity a useful value to track during training.\n",
    "        It indicates how many codes are 'active' on average.\n",
    "        \"\"\"\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(2, 0, 1).contiguous(), perplexity, encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder & Decoder Architecture\n",
    "\n",
    "The encoder and decoder architecture is based on a ResNet and is implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        2 preprocessing convolution layers with filter length 3\n",
    "        and residual connections.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        self._conv_1 = nn.Conv1d(in_channels= 1025, #??features_filters,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=3,\n",
    "                                 padding=1)\n",
    "        \n",
    "        self._conv_2 = nn.Conv1d(in_channels=num_hiddens,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=3,\n",
    "                                 padding=1)\n",
    "        \"\"\"\n",
    "        1 strided convolution length reduction layer with filter\n",
    "        length 4 and stride 2 (downsampling the signal by a factor\n",
    "        of two).\n",
    "        \"\"\"\n",
    "        \n",
    "        self._conv_3 = nn.Conv1d(in_channels=num_hiddens,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=4,\n",
    "                                 stride=1, \n",
    "                                 padding=2)\n",
    "        \"\"\"\n",
    "        2 convolutional layers with length 3 and\n",
    "        residual connections.\n",
    "        \"\"\"\n",
    "        self._conv_4 = nn.Conv1d(in_channels=num_hiddens,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=3,\n",
    "                                 padding=1)\n",
    "        \n",
    "        self._conv_5 = nn.Conv1d(in_channels=num_hiddens,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=3,\n",
    "                                 padding=1)\n",
    "        \n",
    "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
    "                                             num_hiddens=num_hiddens,\n",
    "                                             num_residual_layers=num_residual_layers,\n",
    "                                             num_residual_hiddens=num_residual_hiddens)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        print('shape of inputs in Encoder.forward',inputs.size())\n",
    "\n",
    "        x_conv_1 = F.relu(self._conv_1(inputs))\n",
    "        print('shape of x in Encoder.forward._conv_1',x_conv_1.size())\n",
    "\n",
    "        x = F.relu(self._conv_2(x_conv_1)) + x_conv_1\n",
    "        print('shape of x in Encoder.forward.relu_2',x.size())\n",
    "        \n",
    "        x_conv_3 = F.relu(self._conv_3(x))\n",
    "        print('shape of x in Encoder.forward._conv_3',x.size())\n",
    "        \n",
    "        x_conv_4 = F.relu(self._conv_4(x_conv_3)) + x_conv_3\n",
    "        \n",
    "        x_conv_5 = F.relu(self._conv_5(x_conv_4)) + x_conv_4\n",
    "        \n",
    "        x = self._residual_stack(x_conv_5) + x_conv_5\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
    "        \n",
    "        super(Residual, self).__init__()\n",
    "        \n",
    "        self._block = nn.Sequential(\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Conv1d(\n",
    "                     in_channels=in_channels,\n",
    "                      out_channels=num_residual_hiddens,\n",
    "                      kernel_size=3, \n",
    "                      stride=1, \n",
    "                      padding=1, \n",
    "                      bias=False),\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Conv1d(in_channels=num_residual_hiddens,\n",
    "                      out_channels=num_hiddens,\n",
    "                      kernel_size=1, \n",
    "                      stride=1, \n",
    "                      bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print('shape of x in Residual.forward',x.size())\n",
    "        return x + self._block(x)\n",
    "\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super(ResidualStack, self).__init__()\n",
    "        \n",
    "        self._num_residual_layers = num_residual_layers\n",
    "        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\n",
    "                             for _ in range(self._num_residual_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('shape of x in ResidualStack.forward',x.size())\n",
    "        for i in range(self._num_residual_layers):\n",
    "            x = self._layers[i](x)\n",
    "            print(f'Iteration {i} shape of x in ResidualStack.forward {x.size()} ',)\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self._conv_1 = nn.Conv1d(in_channels=in_channels,\n",
    "                                 out_channels=num_hiddens,\n",
    "                                 kernel_size=3,\n",
    "                                 padding=1)\n",
    "        \n",
    "        self._upsample = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
    "                                             num_hiddens=num_hiddens,\n",
    "                                             num_residual_layers=num_residual_layers,\n",
    "                                             num_residual_hiddens=num_residual_hiddens)\n",
    "        \n",
    "        self._conv_trans_1 = nn.ConvTranspose1d(in_channels=num_hiddens, \n",
    "                                                out_channels=num_hiddens,\n",
    "                                                kernel_size=3, \n",
    "                                                padding=1)\n",
    "        \n",
    "        self._conv_trans_2 = nn.ConvTranspose1d(in_channels=num_hiddens, \n",
    "                                                out_channels=num_hiddens,\n",
    "                                                kernel_size=3,  \n",
    "                                                padding=0)\n",
    "        \n",
    "        self._conv_trans_3 = nn.ConvTranspose1d(in_channels=num_hiddens, \n",
    "                                                out_channels=3,#out_channels\n",
    "                                                kernel_size=2, \n",
    "                                                padding=0)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self._conv_1(inputs)\n",
    "        \n",
    "        x = self._upsample(x)\n",
    "        \n",
    "        x = self._residual_stack(x)\n",
    "        \n",
    "        x = self._conv_trans_1(x)\n",
    "        \n",
    "        x = F.relu(self._conv_trans_1(x))\n",
    "        \n",
    "        x = F.relu(self._conv_trans_2(x))\n",
    "        \n",
    "        x = self._conv_trans_3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "We use the hyperparameters from the author's code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_loader = DataLoader(training_data, \n",
    "#                             batch_size=batch_size, \n",
    "#                             shuffle=True,\n",
    "#                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation_loader = DataLoader(validation_data,\n",
    "#                               batch_size=32,\n",
    "#                               shuffle=True,\n",
    "#                               pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens, \n",
    "                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self._encoder = Encoder(1, num_hiddens,\n",
    "                                num_residual_layers, \n",
    "                                num_residual_hiddens)\n",
    "        self._pre_vq_conv = nn.Conv1d(in_channels=num_hiddens, \n",
    "                                      out_channels=embedding_dim,\n",
    "                                      kernel_size=1, \n",
    "                                      stride=1)\n",
    "        if decay > 0.0:\n",
    "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim, \n",
    "                                              commitment_cost, decay)\n",
    "        else:\n",
    "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
    "                                           commitment_cost, device)\n",
    "        self._decoder = Decoder(embedding_dim,\n",
    "                                num_hiddens, \n",
    "                                num_residual_layers, \n",
    "                                num_residual_hiddens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Model::forward\")\n",
    "        #print(x.size())\n",
    "        z = self._encoder(x)\n",
    "        z = self._pre_vq_conv(z)\n",
    "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
    "        #print(\"quantized \")\n",
    "        #print( quantized.size())\n",
    "        x_recon = self._decoder(quantized)\n",
    "\n",
    "        return loss, x_recon, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "              num_embeddings, embedding_dim, \n",
    "              commitment_cost, decay).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model::forward\n",
      "shape of inputs in Encoder.forward torch.Size([2, 1025, 326])\n",
      "shape of x in Encoder.forward._conv_1 torch.Size([2, 768, 326])\n",
      "shape of x in Encoder.forward.relu_2 torch.Size([2, 768, 326])\n",
      "shape of x in Encoder.forward._conv_3 torch.Size([2, 768, 326])\n",
      "shape of x in ResidualStack.forward torch.Size([2, 768, 327])\n",
      "shape of x in Residual.forward torch.Size([2, 768, 327])\n",
      "Iteration 0 shape of x in ResidualStack.forward torch.Size([2, 768, 327]) \n",
      "shape of x in Residual.forward torch.Size([2, 768, 327])\n",
      "Iteration 1 shape of x in ResidualStack.forward torch.Size([2, 768, 327]) \n",
      "shape of inputs in VectorQuantizer.forward torch.Size([64, 327, 2])\n",
      "shape of flat_input in VectorQuantizer.forward torch.Size([654, 64])\n",
      "device of flat_input cuda:0\n",
      "shape of quantized in VectorQuantizer.forward torch.Size([64, 327, 2])\n",
      "shape of x in ResidualStack.forward torch.Size([2, 768, 654])\n",
      "shape of x in Residual.forward torch.Size([2, 768, 654])\n",
      "Iteration 0 shape of x in ResidualStack.forward torch.Size([2, 768, 654]) \n",
      "shape of x in Residual.forward torch.Size([2, 768, 654])\n",
      "Iteration 1 shape of x in ResidualStack.forward torch.Size([2, 768, 654]) \n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 768, 326]       2,362,368\n",
      "            Conv1d-2             [-1, 768, 326]       1,770,240\n",
      "            Conv1d-3             [-1, 768, 327]       2,360,064\n",
      "            Conv1d-4             [-1, 768, 327]       1,770,240\n",
      "            Conv1d-5             [-1, 768, 327]       1,770,240\n",
      "              ReLU-6             [-1, 768, 327]               0\n",
      "            Conv1d-7              [-1, 32, 327]          73,728\n",
      "              ReLU-8              [-1, 32, 327]               0\n",
      "            Conv1d-9             [-1, 768, 327]          24,576\n",
      "         Residual-10             [-1, 768, 327]               0\n",
      "             ReLU-11             [-1, 768, 327]               0\n",
      "           Conv1d-12              [-1, 32, 327]          73,728\n",
      "             ReLU-13              [-1, 32, 327]               0\n",
      "           Conv1d-14             [-1, 768, 327]          24,576\n",
      "         Residual-15             [-1, 768, 327]               0\n",
      "    ResidualStack-16             [-1, 768, 327]               0\n",
      "          Encoder-17             [-1, 768, 327]               0\n",
      "           Conv1d-18              [-1, 64, 327]          49,216\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-aaa89d5807d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m326\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     91\u001b[0m         )\n\u001b[1;32m     92\u001b[0m         \u001b[0mtotal_params\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nb_params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtotal_output\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"trainable\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trainable\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2770\u001b[0m     \"\"\"\n\u001b[1;32m   2771\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2772\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'list'"
     ]
    }
   ],
   "source": [
    "summary(model, (1025, 326))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in training_loader:\n",
    "    train = i\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1025, 326])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(1, num_hiddens,\n",
    "                                num_residual_layers, \n",
    "                                num_residual_hiddens).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.9948e+00, -5.9375e+00, -5.9967e+00,  ...,  1.7835e-02,\n",
       "           1.7835e-02,  1.7835e-02],\n",
       "         [-1.3668e+01, -6.0679e+00, -7.2850e+00,  ...,  1.2581e-02,\n",
       "           1.2581e-02,  1.2581e-02],\n",
       "         [-1.0343e+01, -1.0765e+01, -1.0108e+01,  ..., -1.7396e-02,\n",
       "          -1.7396e-02, -1.7396e-02],\n",
       "         ...,\n",
       "         [ 1.8608e+01,  1.1135e+01,  1.0847e+01,  ...,  7.2693e-03,\n",
       "           7.2693e-03,  7.2693e-03],\n",
       "         [ 2.4501e+01,  2.7265e+01,  2.7245e+01,  ...,  1.4419e-02,\n",
       "           1.4419e-02,  1.4419e-02],\n",
       "         [-1.1214e+01, -2.7150e+01, -2.4559e+01,  ..., -1.2557e-02,\n",
       "          -1.2557e-02, -1.2557e-02]],\n",
       "\n",
       "        [[-3.4664e+00, -4.9100e+00, -4.9295e+00,  ...,  1.7835e-02,\n",
       "           1.7835e-02,  1.7835e-02],\n",
       "         [-1.7095e+01, -8.5921e+00, -8.2084e+00,  ...,  1.2581e-02,\n",
       "           1.2581e-02,  1.2581e-02],\n",
       "         [-1.1602e+01, -1.1028e+01, -1.0709e+01,  ..., -1.7396e-02,\n",
       "          -1.7396e-02, -1.7396e-02],\n",
       "         ...,\n",
       "         [ 1.5291e+01,  9.4907e+00,  1.0019e+01,  ...,  7.2693e-03,\n",
       "           7.2693e-03,  7.2693e-03],\n",
       "         [ 2.3509e+01,  2.5875e+01,  2.5754e+01,  ...,  1.4419e-02,\n",
       "           1.4419e-02,  1.4419e-02],\n",
       "         [-9.9986e+00, -2.4861e+01, -2.4163e+01,  ..., -1.2557e-02,\n",
       "          -1.2557e-02, -1.2557e-02]],\n",
       "\n",
       "        [[-3.5706e+00, -5.2918e+00, -5.2276e+00,  ...,  1.7835e-02,\n",
       "           1.7835e-02,  1.7835e-02],\n",
       "         [-1.5528e+01, -7.2275e+00, -7.2200e+00,  ...,  1.2581e-02,\n",
       "           1.2581e-02,  1.2581e-02],\n",
       "         [-1.0490e+01, -9.3213e+00, -9.9003e+00,  ..., -1.7396e-02,\n",
       "          -1.7396e-02, -1.7396e-02],\n",
       "         ...,\n",
       "         [ 1.4837e+01,  8.3578e+00,  9.7420e+00,  ...,  7.2693e-03,\n",
       "           7.2693e-03,  7.2693e-03],\n",
       "         [ 2.1707e+01,  2.3002e+01,  2.4242e+01,  ...,  1.4419e-02,\n",
       "           1.4419e-02,  1.4419e-02],\n",
       "         [-8.5474e+00, -2.2437e+01, -2.3024e+01,  ..., -1.2557e-02,\n",
       "          -1.2557e-02, -1.2557e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.8164e+00, -6.3705e+00, -7.9140e+00,  ...,  1.7835e-02,\n",
       "           1.7835e-02,  1.7835e-02],\n",
       "         [-1.8656e+01, -1.0756e+01, -1.0280e+01,  ...,  1.2581e-02,\n",
       "           1.2581e-02,  1.2581e-02],\n",
       "         [-1.2259e+01, -1.2179e+01, -1.2425e+01,  ..., -1.7396e-02,\n",
       "          -1.7396e-02, -1.7396e-02],\n",
       "         ...,\n",
       "         [ 1.7652e+01,  1.0413e+01,  1.0693e+01,  ...,  7.2693e-03,\n",
       "           7.2693e-03,  7.2693e-03],\n",
       "         [ 2.7065e+01,  2.8810e+01,  2.8603e+01,  ...,  1.4419e-02,\n",
       "           1.4419e-02,  1.4419e-02],\n",
       "         [-1.1390e+01, -2.6167e+01, -2.6448e+01,  ..., -1.2557e-02,\n",
       "          -1.2557e-02, -1.2557e-02]],\n",
       "\n",
       "        [[-4.0477e+00, -7.8548e+00, -7.0228e+00,  ...,  1.7835e-02,\n",
       "           1.7835e-02,  1.7835e-02],\n",
       "         [-1.4863e+01, -1.0257e+01, -9.2493e+00,  ...,  1.2581e-02,\n",
       "           1.2581e-02,  1.2581e-02],\n",
       "         [-1.2203e+01, -1.2074e+01, -1.0882e+01,  ..., -1.7396e-02,\n",
       "          -1.7396e-02, -1.7396e-02],\n",
       "         ...,\n",
       "         [ 1.5848e+01,  9.8257e+00,  1.0618e+01,  ...,  7.2693e-03,\n",
       "           7.2693e-03,  7.2693e-03],\n",
       "         [ 2.5431e+01,  2.7801e+01,  2.7435e+01,  ...,  1.4419e-02,\n",
       "           1.4419e-02,  1.4419e-02],\n",
       "         [-1.0794e+01, -2.4433e+01, -2.4029e+01,  ..., -1.2557e-02,\n",
       "          -1.2557e-02, -1.2557e-02]],\n",
       "\n",
       "        [[-3.0775e+00, -4.0241e+00, -4.5462e+00,  ...,  1.7835e-02,\n",
       "           1.7835e-02,  1.7835e-02],\n",
       "         [-1.5471e+01, -8.0775e+00, -8.0643e+00,  ...,  1.2581e-02,\n",
       "           1.2581e-02,  1.2581e-02],\n",
       "         [-1.0234e+01, -1.1414e+01, -1.0774e+01,  ..., -1.7396e-02,\n",
       "          -1.7396e-02, -1.7396e-02],\n",
       "         ...,\n",
       "         [ 1.5522e+01,  8.8163e+00,  9.5825e+00,  ...,  7.2693e-03,\n",
       "           7.2693e-03,  7.2693e-03],\n",
       "         [ 2.2836e+01,  2.5594e+01,  2.6028e+01,  ...,  1.4419e-02,\n",
       "           1.4419e-02,  1.4419e-02],\n",
       "         [-9.7031e+00, -2.2609e+01, -2.3039e+01,  ..., -1.2557e-02,\n",
       "          -1.2557e-02, -1.2557e-02]]], device='cuda:0',\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc._conv_1(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of inputs in Encoder.forward torch.Size([10, 1025, 326])\n",
      "shape of x in Encoder.forward._conv_1 torch.Size([10, 768, 326])\n",
      "shape of x in Encoder.forward.relu_2 torch.Size([10, 768, 326])\n",
      "shape of x in Encoder.forward._conv_3 torch.Size([10, 768, 326])\n",
      "shape of x in ResidualStack.forward torch.Size([10, 768, 327])\n",
      "shape of x in Residual.forward torch.Size([10, 768, 327])\n",
      "Iteration 0 shape of x in ResidualStack.forward torch.Size([10, 768, 327]) \n",
      "shape of x in Residual.forward torch.Size([10, 768, 327])\n",
      "Iteration 1 shape of x in ResidualStack.forward torch.Size([10, 768, 327]) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768, 327])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.forward(train[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model::forward\n",
      "shape of inputs in Encoder.forward torch.Size([10, 1025, 326])\n",
      "shape of x in Encoder.forward._conv_1 torch.Size([10, 768, 326])\n",
      "shape of x in Encoder.forward.relu_2 torch.Size([10, 768, 326])\n",
      "shape of x in Encoder.forward._conv_3 torch.Size([10, 768, 326])\n",
      "shape of x in ResidualStack.forward torch.Size([10, 768, 327])\n",
      "shape of x in Residual.forward torch.Size([10, 768, 327])\n",
      "Iteration 0 shape of x in ResidualStack.forward torch.Size([10, 768, 327]) \n",
      "shape of x in Residual.forward torch.Size([10, 768, 327])\n",
      "Iteration 1 shape of x in ResidualStack.forward torch.Size([10, 768, 327]) \n",
      "shape of x in ResidualStack.forward torch.Size([10, 768, 654])\n",
      "shape of x in Residual.forward torch.Size([10, 768, 654])\n",
      "Iteration 0 shape of x in ResidualStack.forward torch.Size([10, 768, 654]) \n",
      "shape of x in Residual.forward torch.Size([10, 768, 654])\n",
      "Iteration 1 shape of x in ResidualStack.forward torch.Size([10, 768, 654]) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(111.5821, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 2.6973e-03,  3.6762e-03, -1.4785e-03,  ..., -5.4070e-03,\n",
       "            4.0411e-03,  4.1874e-03],\n",
       "          [-1.5532e-02, -8.3419e-03, -1.3484e-02,  ..., -3.4968e-03,\n",
       "           -5.1036e-03, -1.3369e-02],\n",
       "          [-2.6497e-02, -2.9566e-02, -4.2982e-02,  ..., -3.4109e-02,\n",
       "           -2.7443e-02, -2.0236e-02],\n",
       "          ...,\n",
       "          [-2.6597e-02, -2.4212e-02, -2.8920e-02,  ..., -3.3386e-02,\n",
       "           -2.9299e-02, -1.8367e-02],\n",
       "          [-1.2812e-03,  8.1935e-04,  5.5526e-03,  ..., -2.4481e-03,\n",
       "           -1.1364e-05, -8.1752e-03],\n",
       "          [ 9.1256e-03,  1.8751e-02,  1.9326e-02,  ...,  1.8490e-02,\n",
       "            2.2438e-02,  1.6414e-02]],\n",
       " \n",
       "         [[ 2.0671e-03,  4.5945e-03, -4.1662e-04,  ..., -3.0963e-03,\n",
       "            4.7270e-03,  5.1625e-03],\n",
       "          [-1.3824e-02, -5.0775e-03, -1.2608e-02,  ..., -5.0051e-03,\n",
       "           -5.6270e-03, -1.3650e-02],\n",
       "          [-2.4458e-02, -2.4846e-02, -4.0834e-02,  ..., -3.3022e-02,\n",
       "           -2.9006e-02, -2.2449e-02],\n",
       "          ...,\n",
       "          [-2.6500e-02, -2.5611e-02, -2.9547e-02,  ..., -3.2358e-02,\n",
       "           -2.8798e-02, -1.8636e-02],\n",
       "          [-9.0500e-04,  9.1731e-04,  4.6694e-03,  ...,  5.6274e-04,\n",
       "            1.3485e-04, -8.3388e-03],\n",
       "          [ 7.6338e-03,  1.8290e-02,  1.9598e-02,  ...,  2.2436e-02,\n",
       "            2.5986e-02,  1.6319e-02]],\n",
       " \n",
       "         [[ 1.7623e-03,  2.7281e-03, -2.4613e-03,  ..., -3.7987e-03,\n",
       "            3.2818e-03,  4.4553e-03],\n",
       "          [-1.5449e-02, -7.5359e-03, -1.2158e-02,  ..., -2.8643e-03,\n",
       "           -6.6159e-03, -1.3438e-02],\n",
       "          [-2.5423e-02, -2.8606e-02, -4.0902e-02,  ..., -3.3402e-02,\n",
       "           -2.9664e-02, -2.1722e-02],\n",
       "          ...,\n",
       "          [-2.7289e-02, -2.6076e-02, -2.9852e-02,  ..., -2.8456e-02,\n",
       "           -2.8626e-02, -2.0025e-02],\n",
       "          [-5.5519e-04,  1.4891e-03,  4.1153e-03,  ...,  6.5575e-04,\n",
       "            7.7054e-04, -9.2314e-03],\n",
       "          [ 8.2403e-03,  1.6867e-02,  1.7881e-02,  ...,  1.9604e-02,\n",
       "            2.2392e-02,  1.6408e-02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.3771e-03,  1.9080e-03, -6.5190e-04,  ..., -3.2469e-03,\n",
       "            3.1838e-03,  4.2692e-03],\n",
       "          [-1.3999e-02, -7.3360e-03, -1.1586e-02,  ..., -6.6365e-03,\n",
       "           -7.1238e-03, -1.4621e-02],\n",
       "          [-2.5911e-02, -2.8490e-02, -4.1303e-02,  ..., -2.9837e-02,\n",
       "           -2.6604e-02, -2.0965e-02],\n",
       "          ...,\n",
       "          [-2.6491e-02, -2.6071e-02, -3.3253e-02,  ..., -2.9092e-02,\n",
       "           -2.8272e-02, -1.9877e-02],\n",
       "          [-9.5694e-04,  1.1234e-03,  4.1068e-03,  ...,  2.5049e-03,\n",
       "           -2.4274e-04, -8.3533e-03],\n",
       "          [ 9.0643e-03,  1.8015e-02,  2.0040e-02,  ...,  2.0260e-02,\n",
       "            2.1623e-02,  1.6070e-02]],\n",
       " \n",
       "         [[ 2.0359e-03,  5.1927e-03, -6.6999e-06,  ...,  5.4210e-04,\n",
       "            6.3792e-03,  5.1794e-03],\n",
       "          [-1.5636e-02, -6.7875e-03, -1.1147e-02,  ..., -3.8613e-03,\n",
       "           -5.9204e-03, -1.3644e-02],\n",
       "          [-2.6009e-02, -2.8258e-02, -4.2801e-02,  ..., -3.0913e-02,\n",
       "           -2.6368e-02, -2.1046e-02],\n",
       "          ...,\n",
       "          [-2.5829e-02, -2.6292e-02, -2.8668e-02,  ..., -2.9870e-02,\n",
       "           -2.8819e-02, -1.7999e-02],\n",
       "          [-1.7847e-03, -1.1573e-03,  4.8237e-03,  ...,  3.3856e-03,\n",
       "            5.5181e-06, -9.3355e-03],\n",
       "          [ 8.7688e-03,  1.5699e-02,  1.6660e-02,  ...,  2.1450e-02,\n",
       "            2.4497e-02,  1.6062e-02]],\n",
       " \n",
       "         [[ 9.5975e-04,  3.7609e-03, -3.2925e-03,  ..., -3.2765e-03,\n",
       "            2.9360e-03,  4.9458e-03],\n",
       "          [-1.3552e-02, -6.9771e-03, -1.3040e-02,  ..., -4.4317e-03,\n",
       "           -6.5004e-03, -1.3097e-02],\n",
       "          [-2.5791e-02, -2.7676e-02, -4.0024e-02,  ..., -3.1584e-02,\n",
       "           -2.7387e-02, -2.1658e-02],\n",
       "          ...,\n",
       "          [-2.5035e-02, -2.3493e-02, -2.8468e-02,  ..., -3.2945e-02,\n",
       "           -2.9605e-02, -1.8540e-02],\n",
       "          [-9.7572e-04,  1.6985e-03,  6.0922e-03,  ..., -1.8218e-03,\n",
       "           -8.4057e-05, -5.9050e-03],\n",
       "          [ 9.2736e-03,  1.8631e-02,  2.1405e-02,  ...,  2.2826e-02,\n",
       "            2.3505e-02,  1.4778e-02]]], device='cuda:0',\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor(18.1709, device='cuda:0'))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_vq_conv = nn.Conv1d(in_channels=num_hiddens, \n",
    "                                      out_channels=embedding_dim,\n",
    "                                      kernel_size=1, \n",
    "                                      stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of inputs in Encoder.forward torch.Size([10, 1025, 326])\n",
      "shape of x in Encoder.forward._conv_1 torch.Size([10, 768, 326])\n",
      "shape of x in Encoder.forward.relu_2 torch.Size([10, 768, 326])\n",
      "shape of x in Encoder.forward._conv_3 torch.Size([10, 768, 326])\n",
      "shape of x in ResidualStack.forward torch.Size([10, 768, 327])\n",
      "shape of x in Residual.forward torch.Size([10, 768, 327])\n",
      "Iteration 0 shape of x in ResidualStack.forward torch.Size([10, 768, 327]) \n",
      "shape of x in Residual.forward torch.Size([10, 768, 327])\n",
      "Iteration 1 shape of x in ResidualStack.forward torch.Size([10, 768, 327]) \n",
      "shape of x in ResidualStack.forward torch.Size([10, 768, 654])\n",
      "shape of x in Residual.forward torch.Size([10, 768, 654])\n",
      "Iteration 0 shape of x in ResidualStack.forward torch.Size([10, 768, 654]) \n",
      "shape of x in Residual.forward torch.Size([10, 768, 654])\n",
      "Iteration 1 shape of x in ResidualStack.forward torch.Size([10, 768, 654]) \n"
     ]
    }
   ],
   "source": [
    "x = model._encoder(train[0])\n",
    "x = model._pre_vq_conv(x)\n",
    "_,x,_,_ = model._vq_vae(x)\n",
    "x = model._decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 5 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-986ea076c0b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m vq_vae = VectorQuantizer(num_embeddings, embedding_dim, \n\u001b[0;32m----> 2\u001b[0;31m                                               commitment_cost, decay, device)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 5 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "vq_vae = VectorQuantizer(num_embeddings, embedding_dim, \n",
    "                                              commitment_cost, decay, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of inputs in Encoder.forward torch.Size([10, 1025, 326])\n",
      "shape of x in Encoder.forward._conv_1 torch.Size([10, 768, 326])\n",
      "shape of x in Encoder.forward.relu_2 torch.Size([10, 768, 326])\n",
      "shape of x in Encoder.forward._conv_3 torch.Size([10, 768, 326])\n",
      "shape of x in ResidualStack.forward torch.Size([10, 768, 327])\n",
      "shape of x in Residual.forward torch.Size([10, 768, 327])\n",
      "Iteration 0 shape of x in ResidualStack.forward torch.Size([10, 768, 327]) \n",
      "shape of x in Residual.forward torch.Size([10, 768, 327])\n",
      "Iteration 1 shape of x in ResidualStack.forward torch.Size([10, 768, 327]) \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected device cuda:0 but got device cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-33df006c835c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvq_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-525844c22ba6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, compute_distances_if_possible, record_codebook_stats)\u001b[0m\n\u001b[1;32m     77\u001b[0m         distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n\u001b[1;32m     78\u001b[0m                     \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n\u001b[0m\u001b[1;32m     80\u001b[0m         \"\"\"\n\u001b[1;32m     81\u001b[0m         \u001b[0mencoding_indices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdiscrete\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected device cuda:0 but got device cpu"
     ]
    }
   ],
   "source": [
    "vq_vae.forward(enc.forward(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorQuantizerEMA(\n",
       "  (_embedding): Embedding(29, 64)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vq_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim, \n",
    "                                              commitment_cost, decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "res  = ResidualStack(in_channels=num_hiddens,\n",
    "                                             num_hiddens=num_hiddens,\n",
    "                                             num_residual_layers=num_residual_layers,\n",
    "                                             num_residual_hiddens=num_residual_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-dcb283ac43f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "res.forward(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768, 656])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1025, 326])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model::forward\n",
      "shape of inputs in Encoder.forward torch.Size([2, 768, 1025, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional input for 3-dimensional weight 768 1025 3, but got 4-dimensional input of size [2, 768, 1025, 3] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-1700bf647d8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-772dc7c09c80>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model::forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#print(x.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_vq_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vq_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-6be3e39a5ecb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape of inputs in Encoder.forward'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mx_conv_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape of x in Encoder.forward._conv_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_conv_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    201\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 202\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3-dimensional input for 3-dimensional weight 768 1025 3, but got 4-dimensional input of size [2, 768, 1025, 3] instead"
     ]
    }
   ],
   "source": [
    "summary(model,(768, 1025, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size\n",
      "torch.Size([10, 1025, 326])\n",
      "Model::forward\n",
      "shape of inputs in Encoder.forward torch.Size([10, 1025, 326])\n",
      "shape of x in Encoder.forward._conv_1 torch.Size([10, 768, 326])\n",
      "shape of x in Encoder.forward.relu_2 torch.Size([10, 768, 326])\n",
      "shape of x in Encoder.forward._conv_3 torch.Size([10, 768, 326])\n",
      "shape of x in ResidualStack.forward torch.Size([10, 768, 327])\n",
      "shape of x in Residual.forward torch.Size([10, 768, 327])\n",
      "Iteration 0 shape of x in ResidualStack.forward torch.Size([10, 768, 327]) \n",
      "shape of x in Residual.forward torch.Size([10, 768, 327])\n",
      "Iteration 1 shape of x in ResidualStack.forward torch.Size([10, 768, 327]) \n",
      "shape of x in ResidualStack.forward torch.Size([10, 768, 654])\n",
      "shape of x in Residual.forward torch.Size([10, 768, 654])\n",
      "Iteration 0 shape of x in ResidualStack.forward torch.Size([10, 768, 654]) \n",
      "shape of x in Residual.forward torch.Size([10, 768, 654])\n",
      "Iteration 1 shape of x in ResidualStack.forward torch.Size([10, 768, 654]) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Using a target size (torch.Size([10, 1025, 326])) that is different to the input size (torch.Size([10, 768, 656])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (656) must match the size of tensor b (326) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-415e6dd815ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvq_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mrecon_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdata_variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecon_error\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvq_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (656) must match the size of tensor b (326) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "train_res_recon_error = []\n",
    "train_res_perplexity = []\n",
    "\n",
    "for i in xrange(num_training_updates):\n",
    "    (data, _) = next(iter(training_loader))\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Data size\")\n",
    "    print(data.size())\n",
    "\n",
    "    vq_loss, data_recon, perplexity = model(data)\n",
    "    recon_error = F.mse_loss(data_recon, data) / data_variance\n",
    "    loss = recon_error + vq_loss\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_res_recon_error.append(recon_error.item())\n",
    "    train_res_perplexity.append(perplexity.item())\n",
    "\n",
    "    if (i+1) % 100 == 0:\n",
    "        print('%d iterations' % (i+1))\n",
    "        print('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))\n",
    "        print('perplexity: %.3f' % np.mean(train_res_perplexity[-100:]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
